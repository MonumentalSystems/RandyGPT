{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-md-title",
   "metadata": {},
   "source": [
    "# randyGPT — PyTorch Training on Colab GPU\n",
    "\n",
    "Trains a randyGPT model on NVIDIA GPU (T4/V100/A100) and saves RGPT0003-compatible checkpoints\n",
    "that can be loaded directly by the Rust CPU inference server.\n",
    "\n",
    "**Setup:** Runtime → Change runtime type → GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1-gpu-check",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Cell 1: GPU check + dependencies ──────────────────────────────────────────\n",
    "\n",
    "import subprocess, sys\n",
    "\n",
    "# Show GPU info\n",
    "result = subprocess.run(['nvidia-smi', '--query-gpu=name,memory.total', '--format=csv,noheader'],\n",
    "                        capture_output=True, text=True)\n",
    "if result.returncode == 0:\n",
    "    gpu_info = result.stdout.strip()\n",
    "    print(f'GPU: {gpu_info}')\n",
    "else:\n",
    "    print('WARNING: No GPU detected! Go to Runtime > Change runtime type > GPU')\n",
    "\n",
    "# Install dependencies\n",
    "!pip install -q safetensors transformers\n",
    "\n",
    "import torch\n",
    "print(f'PyTorch: {torch.__version__}')\n",
    "print(f'CUDA available: {torch.cuda.is_available()}')\n",
    "if torch.cuda.is_available():\n",
    "    print(f'GPU: {torch.cuda.get_device_name(0)}')\n",
    "\n",
    "# ── Estimated iteration times by GPU and model size ───────────────────────────\n",
    "print('''\n",
    "Estimated ms/iter (batch=64, block=256, BPE):\n",
    "┌────────┬──────────┬───────────┬───────────┐\n",
    "│ Model  │ T4 fp16  │ V100 fp16 │ A100 bf16 │\n",
    "├────────┼──────────┼───────────┼───────────┤\n",
    "│ xs     │ ~250ms   │ ~120ms    │ ~35ms     │\n",
    "│ s      │ ~500ms   │ ~240ms    │ ~70ms     │\n",
    "│ ds     │ ~800ms   │ ~380ms    │ ~110ms    │\n",
    "│ l      │ ~1600ms  │ ~760ms    │ ~220ms    │\n",
    "└────────┴──────────┴───────────┴───────────┘\n",
    "1000 iters on T4 (model-s, fp16): ~8 minutes\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2-drive",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Cell 2: Mount Google Drive ────────────────────────────────────────────────\n",
    "#\n",
    "# Drive is used to:\n",
    "#   - Persist checkpoints across Colab sessions (auto-copy on new best val loss)\n",
    "#   - Upload train.txt / vocab.json from your local machine\n",
    "#   - Resume training after session timeout\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "import os\n",
    "DRIVE_DIR = '/content/drive/MyDrive/randyGPT'\n",
    "os.makedirs(DRIVE_DIR, exist_ok=True)\n",
    "print(f'Drive directory ready: {DRIVE_DIR}')\n",
    "print(f'Contents: {os.listdir(DRIVE_DIR)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3-upload",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Cell 3: Upload files ──────────────────────────────────────────────────────\n",
    "#\n",
    "# Option A (recommended): Copy from Drive if you already uploaded there\n",
    "#   !cp \"$DRIVE_DIR/train.txt\" /content/train.txt\n",
    "#   !cp \"$DRIVE_DIR/vocab.json\" /content/vocab.json\n",
    "#\n",
    "# Option B: Upload directly via file picker\n",
    "#   from google.colab import files\n",
    "#   files.upload()  # select train.txt and vocab.json\n",
    "#\n",
    "# Option C (for scripts): Clone or upload the randyGPT repo\n",
    "#   !git clone https://github.com/yourname/randyGPT /content/randyGPT\n",
    "#   OR upload the scripts/ directory manually\n",
    "\n",
    "import sys, os\n",
    "\n",
    "# ── Copy training data from Drive ─────────────────────────────────────────────\n",
    "!cp \"$DRIVE_DIR/train.txt\" /content/train.txt 2>/dev/null || echo 'train.txt not in Drive — upload it'\n",
    "!cp \"$DRIVE_DIR/vocab.json\" /content/vocab.json 2>/dev/null || echo 'vocab.json not in Drive — upload it'\n",
    "\n",
    "# ── Verify scripts are available ──────────────────────────────────────────────\n",
    "SCRIPTS_DIR = '/content/randyGPT/scripts'\n",
    "if not os.path.exists(SCRIPTS_DIR):\n",
    "    print(f'Scripts not found at {SCRIPTS_DIR}')\n",
    "    print('Upload or clone the randyGPT repo:')\n",
    "    print('  !git clone https://github.com/yourname/randyGPT /content/randyGPT')\n",
    "else:\n",
    "    sys.path.insert(0, SCRIPTS_DIR)\n",
    "    print(f'Scripts ready: {os.listdir(SCRIPTS_DIR)}')\n",
    "\n",
    "# ── Check data ─────────────────────────────────────────────────────────────────\n",
    "for f in ['/content/train.txt', '/content/vocab.json']:\n",
    "    if os.path.exists(f):\n",
    "        size_mb = os.path.getsize(f) / 1e6\n",
    "        print(f'  ✓ {f} ({size_mb:.1f} MB)')\n",
    "    else:\n",
    "        print(f'  ✗ {f} MISSING')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4-config",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Cell 4: Training configuration ───────────────────────────────────────────\n",
    "#\n",
    "# Edit these values, then run Cell 5 to start training.\n",
    "\n",
    "MODEL_SIZE  = 's'        # xs / s / ds / m / l / deep / xl\n",
    "ITERS       = 1000       # total training iterations\n",
    "DTYPE       = 'bf16'     # bf16 (A100) / fp16 (T4/V100) / fp32 (CPU debug)\n",
    "BATCH_SIZE  = 64         # per-step batch size\n",
    "GRAD_ACCUM  = 1          # gradient accumulation steps (effective batch = BATCH_SIZE * GRAD_ACCUM)\n",
    "\n",
    "# Set to a .bin or .safetensors path to resume from a previous checkpoint:\n",
    "RESUME      = ''         # e.g. '/content/drive/MyDrive/randyGPT/checkpoint_best.bin'\n",
    "\n",
    "TRAIN_FILE  = '/content/train.txt'\n",
    "VOCAB_FILE  = '/content/vocab.json'\n",
    "OUTPUT_DIR  = '/content/output'\n",
    "SCRIPTS_DIR = '/content/randyGPT/scripts'\n",
    "\n",
    "# Drive backup: copies checkpoint_best.bin here after each new best val loss\n",
    "DRIVE_BACKUP = f'{DRIVE_DIR}/checkpoint_best.bin'\n",
    "\n",
    "import os\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Build the command\n",
    "cmd_parts = [\n",
    "    f'python {SCRIPTS_DIR}/train_torch.py',\n",
    "    f'--model-size {MODEL_SIZE}',\n",
    "    f'--iters {ITERS}',\n",
    "    '--bpe',\n",
    "    f'--batch-size {BATCH_SIZE}',\n",
    "    f'--grad-accum {GRAD_ACCUM}',\n",
    "    f'--dtype {DTYPE}',\n",
    "    f'--train-file {TRAIN_FILE}',\n",
    "    f'--vocab {VOCAB_FILE}',\n",
    "    f'--output {OUTPUT_DIR}',\n",
    "    f'--drive {DRIVE_BACKUP}',\n",
    "]\n",
    "if RESUME:\n",
    "    cmd_parts.append(f'--resume {RESUME}')\n",
    "\n",
    "TRAIN_CMD = ' '.join(cmd_parts)\n",
    "print('Training command:')\n",
    "print(TRAIN_CMD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5-train",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Cell 5: Run training ──────────────────────────────────────────────────────\n",
    "#\n",
    "# Output: checkpoint.bin and checkpoint_best.bin in OUTPUT_DIR\n",
    "#         checkpoint_best.bin also copied to DRIVE_BACKUP after each new best\n",
    "#\n",
    "# If the Colab session disconnects, re-run Cells 1-4 then resume:\n",
    "#   RESUME = f'{DRIVE_DIR}/checkpoint_best.bin'\n",
    "\n",
    "import subprocess, sys\n",
    "\n",
    "proc = subprocess.Popen(\n",
    "    TRAIN_CMD.split(),\n",
    "    stdout=subprocess.PIPE,\n",
    "    stderr=subprocess.STDOUT,\n",
    "    text=True,\n",
    "    bufsize=1,\n",
    ")\n",
    "for line in proc.stdout:\n",
    "    print(line, end='', flush=True)\n",
    "proc.wait()\n",
    "print(f'\\nExit code: {proc.returncode}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6-export",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Cell 6: Export to HuggingFace format ──────────────────────────────────────\n",
    "#\n",
    "# Produces: OUTPUT_DIR/hf_export/ with config.json, model.safetensors, tokenizer.json\n",
    "\n",
    "HF_OUTPUT = f'{OUTPUT_DIR}/hf_export'\n",
    "BEST_CKPT = f'{OUTPUT_DIR}/checkpoint_best.bin'\n",
    "\n",
    "!python {SCRIPTS_DIR}/export_hf.py \\\n",
    "    --checkpoint {BEST_CKPT} \\\n",
    "    --vocab {VOCAB_FILE} \\\n",
    "    --output {HF_OUTPUT} \\\n",
    "    --model-size {MODEL_SIZE}\n",
    "\n",
    "import os\n",
    "print('\\nExported files:')\n",
    "for f in sorted(os.listdir(HF_OUTPUT)):\n",
    "    size = os.path.getsize(f'{HF_OUTPUT}/{f}')\n",
    "    print(f'  {f} ({size/1e3:.1f} KB)')\n",
    "\n",
    "# Copy HF export to Drive for safekeeping\n",
    "import shutil\n",
    "hf_drive = f'{DRIVE_DIR}/hf_export'\n",
    "if os.path.exists(hf_drive):\n",
    "    shutil.rmtree(hf_drive)\n",
    "shutil.copytree(HF_OUTPUT, hf_drive)\n",
    "print(f'\\n✓ HF export copied to Drive: {hf_drive}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7-gen",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Cell 7: Quick generation test ─────────────────────────────────────────────\n",
    "#\n",
    "# Load the exported model and generate 200 tokens to verify it works.\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, SCRIPTS_DIR)\n",
    "sys.path.insert(0, HF_OUTPUT)  # for modeling_randygpt.py in the export\n",
    "\n",
    "import torch\n",
    "from safetensors.torch import load_file\n",
    "from modeling_randygpt import RandyGPTConfig, RandyGPTForCausalLM\n",
    "from tokenizer_randygpt import RandyGPTTokenizer\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Load config and model\n",
    "import json\n",
    "cfg_data = json.loads(open(f'{HF_OUTPUT}/config.json').read())\n",
    "cfg = RandyGPTConfig(\n",
    "    vocab_size=cfg_data['vocab_size'],\n",
    "    n_embd=cfg_data['n_embd'],\n",
    "    n_head=cfg_data['n_head'],\n",
    "    n_layer=cfg_data['n_layer'],\n",
    "    block_size=cfg_data['block_size'],\n",
    ")\n",
    "model = RandyGPTForCausalLM(cfg)\n",
    "state = load_file(f'{HF_OUTPUT}/model.safetensors', device=str(device))\n",
    "model.load_state_dict(state, strict=True)\n",
    "model = model.to(device).eval()\n",
    "print(f'Model loaded: {sum(p.numel() for p in model.parameters())/1e6:.2f}M params')\n",
    "\n",
    "# Load tokenizer\n",
    "tok = RandyGPTTokenizer.from_file(VOCAB_FILE)\n",
    "\n",
    "# Generate\n",
    "PROMPTS = [\n",
    "    'Once upon a time',\n",
    "    'The old man',\n",
    "    'It was a dark and stormy night',\n",
    "]\n",
    "\n",
    "for prompt in PROMPTS:\n",
    "    ids     = torch.tensor([tok.encode(prompt)], dtype=torch.long, device=device)\n",
    "    out_ids = model.generate_text(ids, max_new_tokens=200, temperature=0.8, top_p=0.9)\n",
    "    text    = tok.decode(out_ids[0].tolist())\n",
    "    print(f'\\n{\"─\"*60}')\n",
    "    print(f'Prompt: \"{prompt}\"')\n",
    "    print(f'Output:\\n{text}')\n",
    "\n",
    "print(f'\\n{\"─\"*60}')\n",
    "print('Generation test complete.')"
   ]
  }
 ]
}
